{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for CLIP\n",
    "CLIP is a Vision Lanuage Model published by OpenAI. It combines Vision & Language and we can do some cool stuff with it.\n",
    "Here is a demo of what you can for example do with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%pip install torch torchvision\n",
    "%pip install open_clip_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "tokenizer = open_clip.get_tokenizer('ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function for loading an image and pre-processing it\n",
    "def load_and_process_image(image_path):\n",
    "    image = Image.open(str(image_path))\n",
    "    inputs = preprocess(image).unsqueeze(0)\n",
    "    image_features = model.encode_image(inputs)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    return image_features, image\n",
    "\n",
    "# Shortcut function for processing text in CLIP\n",
    "def process_text(texts, formatted=\"this is an image of {}\"):\n",
    "    if formatted:\n",
    "        texts = [formatted.format(c) for c in texts]\n",
    "    texts = tokenizer(texts)\n",
    "\n",
    "    text_features = model.encode_text(texts)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return text_features\n",
    "\n",
    "# Helper to display images\n",
    "def show_image(pil_image, max_size=400):\n",
    "    pil_image.thumbnail((max_size, max_size))\n",
    "    return display(pil_image)\n",
    "\n",
    "# Helper to display the results of classification\n",
    "def print_classes(classes, text_probs):\n",
    "    max_idx = torch.max(text_probs, dim=1)[1].item()\n",
    "    for i in range(len(classes)):\n",
    "        text = f\"- {classes[i]:<12} {text_probs[0, i].item()*100:.2f}%\"\n",
    "        if i == max_idx:\n",
    "            text += \" (best match)\"\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different concepts\n",
    "CLIP can understand vastly different concepts. An example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"a diagram\", \"a dog\", \"a cat\", \"a car\"]\n",
    "text_features = process_text(classes)\n",
    "\n",
    "dir = Path(\"images/start/\")\n",
    "for image_path in dir.glob(\"*\"):\n",
    "    image_features, image = load_and_process_image(image_path)\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    show_image(image)\n",
    "    print_classes(classes, text_probs)\n",
    "    print(\"--------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific concepts\n",
    "It can also be more specific, such as different breeds of cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"European Shorthair\", \"Maine Coon\", \"Siamese\", \"British Shorthair\", \"Persian\"]\n",
    "text_features = process_text(classes, formatted=\"this image contains a {}, a cat breed\")\n",
    "\n",
    "dir = Path(\"images/cat_breeds/\")\n",
    "for image_path in dir.glob(\"*.jpg\"):\n",
    "    image_features, image = load_and_process_image(image_path)\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    show_image(image)\n",
    "    print_classes(classes, text_probs)\n",
    "    print(\"--------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract concepts\n",
    "Also more abstract concepts such as seasons & cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"spring\", \"summer\", \"autumn\", \"winter\"]\n",
    "text_features = process_text(classes, formatted=\"this image is taken in {}\")\n",
    "\n",
    "dir = Path(\"images/seasons/\")\n",
    "for image_path in dir.glob(\"*\"):\n",
    "    image_features, image = load_and_process_image(image_path)\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    show_image(image)\n",
    "    print_classes(classes, text_probs)\n",
    "    print(\"--------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"amsterdam\", \"new york\", \"tokyo\", \"paris\"]\n",
    "text_features = process_text(classes, formatted=\"this image is taken in {}\")\n",
    "\n",
    "dir = Path(\"images/cities/\")\n",
    "for image_path in dir.glob(\"*\"):\n",
    "    image_features, image = load_and_process_image(image_path)\n",
    "    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    show_image(image)\n",
    "    print_classes(classes, text_probs)\n",
    "    print(\"--------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Retrieval\n",
    "We can also do some image retrieval. Select the best images given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = Path(\"images/cats/\")\n",
    "\n",
    "all_images = []\n",
    "all_image_features = []\n",
    "for image_path in tqdm(list(dir.glob(\"*\"))):\n",
    "    image_features, image = load_and_process_image(image_path)\n",
    "    all_images.append(image)\n",
    "    all_image_features.append(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"multiple kittens\"]\n",
    "text_features = process_text(classes, formatted=\"this image contains {}\")\n",
    "\n",
    "probs = (100.0 * torch.concat(all_image_features) @ text_features.T)\n",
    "sorted_indexes = torch.argsort(probs.squeeze(), dim=0, descending=True).tolist()\n",
    "\n",
    "for i in sorted_indexes:\n",
    "    show_image(all_images[i])\n",
    "    print(f\"Similarity: {probs[i].item():.2f}\")\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip_dip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
